{"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using the Wisconsin breast cancer diagnostic data set for predictive analysis\n## Yagmur Onay (04-09-2022)\nAttribute Information:\n\n - 1) ID number \n - 2) Diagnosis (M = malignant, B = benign) \n \n-3-32.Ten real-valued features are computed for each cell nucleus:\n\n - a) radius (mean of distances from center to points on the perimeter) \n - b) texture (standard deviation of gray-scale values) \n - c) perimeter \n - d) area \n - e) smoothness (local variation in radius lengths) \n - f) compactness (perimeter^2 / area - 1.0) \n - g). concavity (severity of concave portions of the contour) \n - h). concave points (number of concave portions of the contour) \n - i). symmetry \n - j). fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.","metadata":{"_cell_guid":"dafef955-4c2c-a871-f1d8-3e0d306393b0"}},{"cell_type":"markdown","source":"## Load Libraries","metadata":{"_cell_guid":"5e26372e-f1bd-b50f-0c1c-33a44306d1f7"}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n%matplotlib inline \nimport matplotlib.pyplot as plt # side-stepping mpl backend\nimport matplotlib.gridspec as gridspec # subplots\nimport mpld3 as mpl\n\n# import models from scikit learn module\nfrom sklearn.model_selection import train_test_split,KFold # For K-fold cross validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import metrics\n\nimport keras\n\nimport tensorflow as tf","metadata":{"_cell_guid":"2768ce80-1a7d-ca31-a35f-29cf0ef7fb15","execution":{"iopub.status.busy":"2022-09-03T22:01:40.983727Z","iopub.execute_input":"2022-09-03T22:01:40.984203Z","iopub.status.idle":"2022-09-03T22:01:40.994595Z","shell.execute_reply.started":"2022-09-03T22:01:40.984162Z","shell.execute_reply":"2022-09-03T22:01:40.993508Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"# Load the data","metadata":{"_cell_guid":"09b9d090-2cba-ad5a-58ce-84208f95dba4"}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/data.csv\",header = 0)\ndf.head()","metadata":{"_cell_guid":"9180cb22-53d2-6bf2-3a29-99448ab808fb","execution":{"iopub.status.busy":"2022-09-03T22:01:41.011906Z","iopub.execute_input":"2022-09-03T22:01:41.012978Z","iopub.status.idle":"2022-09-03T22:01:41.062607Z","shell.execute_reply.started":"2022-09-03T22:01:41.012929Z","shell.execute_reply":"2022-09-03T22:01:41.061412Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"# Clean and prepare data","metadata":{"_cell_guid":"e382010d-1d71-b8d6-4a6e-a0abc9e42372"}},{"cell_type":"code","source":"df.drop('id',axis=1,inplace=True)\ndf.drop('Unnamed: 32',axis=1,inplace=True)\n# size of the dataframe\nlen(df)","metadata":{"_cell_guid":"f9fd3701-af9d-8d8c-5d0e-e2673d7977fe","execution":{"iopub.status.busy":"2022-09-03T22:01:41.064788Z","iopub.execute_input":"2022-09-03T22:01:41.066091Z","iopub.status.idle":"2022-09-03T22:01:41.078679Z","shell.execute_reply.started":"2022-09-03T22:01:41.066038Z","shell.execute_reply":"2022-09-03T22:01:41.077064Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# unique values in the column diagnosis are 'M' for malignant and 'B' for benign.\ndf.diagnosis.unique()","metadata":{"_cell_guid":"083fe464-8dac-713e-d0a1-46435c0d93fa","execution":{"iopub.status.busy":"2022-09-03T22:01:41.080799Z","iopub.execute_input":"2022-09-03T22:01:41.082055Z","iopub.status.idle":"2022-09-03T22:01:41.091299Z","shell.execute_reply.started":"2022-09-03T22:01:41.081969Z","shell.execute_reply":"2022-09-03T22:01:41.089956Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# convert values in  diagnosis column to numerical: map Ms to ones and Bs to zeros\ndf['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})\ndf.head()","metadata":{"_cell_guid":"0882e4c2-3d4d-d4d9-5f49-f36c1b248b93","execution":{"iopub.status.busy":"2022-09-03T22:01:41.099374Z","iopub.execute_input":"2022-09-03T22:01:41.100306Z","iopub.status.idle":"2022-09-03T22:01:41.139334Z","shell.execute_reply.started":"2022-09-03T22:01:41.100246Z","shell.execute_reply":"2022-09-03T22:01:41.138101Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"# Explore data","metadata":{"_cell_guid":"9308c3e3-af06-6f2d-b4cf-f2dc9a47a881"}},{"cell_type":"code","source":"df.describe()","metadata":{"_cell_guid":"cfd882cd-1719-4093-934a-539faf665353","execution":{"iopub.status.busy":"2022-09-03T22:01:41.141812Z","iopub.execute_input":"2022-09-03T22:01:41.142307Z","iopub.status.idle":"2022-09-03T22:01:41.271654Z","shell.execute_reply.started":"2022-09-03T22:01:41.142262Z","shell.execute_reply":"2022-09-03T22:01:41.270236Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"plt.hist(df['diagnosis'])\nplt.title('Diagnosis (M=1 , B=0)')\nplt.show()","metadata":{"_cell_guid":"aa80be8a-4022-038b-d7b7-0789df4ef973","execution":{"iopub.status.busy":"2022-09-03T22:01:41.274278Z","iopub.execute_input":"2022-09-03T22:01:41.274765Z","iopub.status.idle":"2022-09-03T22:01:41.507156Z","shell.execute_reply.started":"2022-09-03T22:01:41.274719Z","shell.execute_reply":"2022-09-03T22:01:41.505992Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"There are about 350 malignant and 220 benign tumor cell data.","metadata":{}},{"cell_type":"markdown","source":"### nucleus features vs diagnosis","metadata":{"_cell_guid":"56b72979-5155-2a99-1b6e-a55cbf72d2a3"}},{"cell_type":"code","source":"features_mean=list(df.columns[1:11])\n\n# split dataframe into two based on diagnosis\ndfM=df[df['diagnosis'] ==1]\ndfB=df[df['diagnosis'] ==0]","metadata":{"_cell_guid":"bc36c937-c5d8-8635-480b-777a94571310","execution":{"iopub.status.busy":"2022-09-03T22:01:41.508987Z","iopub.execute_input":"2022-09-03T22:01:41.509404Z","iopub.status.idle":"2022-09-03T22:01:41.517330Z","shell.execute_reply.started":"2022-09-03T22:01:41.509369Z","shell.execute_reply":"2022-09-03T22:01:41.516102Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"#Stack the data\nplt.rcParams.update({'font.size': 8})\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\naxes = axes.ravel()\nfor idx,ax in enumerate(axes):\n    ax.figure\n    binwidth= (max(df[features_mean[idx]]) - min(df[features_mean[idx]]))/50\n    ax.hist([dfM[features_mean[idx]],dfB[features_mean[idx]]], bins=np.arange(min(df[features_mean[idx]]), max(df[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5,stacked=True, density = True, label=['M','B'],color=['r','g'])\n    ax.legend(loc='upper right')\n    ax.set_title(features_mean[idx])\nplt.tight_layout()\nplt.show()","metadata":{"_cell_guid":"3f3b5e1b-605d-51b4-28c7-c551b5d13a48","execution":{"iopub.status.busy":"2022-09-03T22:01:41.520020Z","iopub.execute_input":"2022-09-03T22:01:41.520711Z","iopub.status.idle":"2022-09-03T22:01:45.325051Z","shell.execute_reply.started":"2022-09-03T22:01:41.520672Z","shell.execute_reply":"2022-09-03T22:01:45.323509Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"### Observations\n\n1. Mean values of cell radius, perimeter, area, compactness, concavity and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors. \n2. Mean values of texture, smoothness, symmetry or fractual dimension don't show a particular preference of one diagnosis over the other. In any of the histograms there are no noticeable large outliers that require further cleanup.","metadata":{"_cell_guid":"4b8d6133-427b-1ecf-0e24-9ec2afea0a0e"}},{"cell_type":"markdown","source":"## Creating a test set and a training set\nDo a simple 80:20 split to create a training data set and a test data set.We won't touch the test set till the final evaluation of the traineed model.","metadata":{"_cell_guid":"ac11039f-0418-3553-9412-ae3d50bef4e4"}},{"cell_type":"code","source":"traindf, testdf = train_test_split(df, test_size = 0.2)","metadata":{"_cell_guid":"1390898b-a338-7395-635f-6e1c216861e6","execution":{"iopub.status.busy":"2022-09-03T22:01:45.327404Z","iopub.execute_input":"2022-09-03T22:01:45.328712Z","iopub.status.idle":"2022-09-03T22:01:45.335882Z","shell.execute_reply.started":"2022-09-03T22:01:45.328653Z","shell.execute_reply":"2022-09-03T22:01:45.334395Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"## Model Classification\n\nHere we are going to build a classification model and evaluate its performance using the test set.","metadata":{"_cell_guid":"45dac047-52fb-b847-a521-fa6883ebd5f6"}},{"cell_type":"code","source":"# Generic function for making a classification model and assesing the performance. \ndef classification_model(model, traindf, predictors, outcome):\n    #Perform k-fold cross-validation with 5 folds\n    kf = KFold(n_splits=5)\n    error = []\n    for train, val in kf.split(traindf):\n\n        # Training x\n        train_predictors = (traindf[predictors].iloc[train,:])    \n        # Training y\n        train_target = traindf[outcome].iloc[train]\n        # Val x\n        val_predictors = (traindf[predictors].iloc[val,:])  \n        # Val y\n        val_target = traindf[outcome].iloc[val]\n\n        # Training the algorithm using the predictors and target.\n        model.fit(train_predictors, train_target)\n\n        # Print error from each cross-validation run\n        score = model.score(val_predictors, val_target)\n        print(\"Cross-Validation Score from current fold: %s\" % \"{0:.3%}\".format(score))\n\n        # Record error from each cross-validation run\n        error.append(model.score(val_predictors, val_target))\n\n    #Make predictions on training set:\n    predictions = model.predict(traindf[predictors])  \n    #Print accuracy\n    accuracy = metrics.accuracy_score(predictions,traindf[outcome])\n    print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))  \n    print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))","metadata":{"_cell_guid":"780b88d4-523e-b3f8-87dc-17c093663f19","execution":{"iopub.status.busy":"2022-09-03T22:01:45.337633Z","iopub.execute_input":"2022-09-03T22:01:45.338149Z","iopub.status.idle":"2022-09-03T22:01:45.352943Z","shell.execute_reply.started":"2022-09-03T22:01:45.338102Z","shell.execute_reply":"2022-09-03T22:01:45.351640Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression model\n\nLogistic regression is widely used for classification of discrete data. In this case we will use it for binary (1,0) classification.\n\nBased on the observations in the histogram plots, we can reasonably hypothesize that the cancer diagnosis depends on the mean cell radius, mean perimeter, mean area, mean compactness, mean concavity and mean concave points. We can then  perform a logistic regression analysis using those features as follows:","metadata":{"_cell_guid":"33763a8f-3917-25c9-b581-d03be3078af2"}},{"cell_type":"code","source":"predictor_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']\noutcome_var='diagnosis'\nmodel=LogisticRegression()\nclassification_model(model,traindf,predictor_var,outcome_var)","metadata":{"_cell_guid":"23d25895-a0ca-c355-899d-4a838e0d799d","execution":{"iopub.status.busy":"2022-09-03T22:01:45.354547Z","iopub.execute_input":"2022-09-03T22:01:45.355338Z","iopub.status.idle":"2022-09-03T22:01:45.494207Z","shell.execute_reply.started":"2022-09-03T22:01:45.355302Z","shell.execute_reply":"2022-09-03T22:01:45.492819Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# Make predictions on test set:\npredictions = model.predict(testdf[predictor_var])  \n# Print accuracy\naccuracy = metrics.accuracy_score(predictions,testdf[outcome_var])\nprint(\"Logistic Regression Test Accuracy : %s\" % \"{0:.3%}\".format(accuracy))  ","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:01:45.496035Z","iopub.execute_input":"2022-09-03T22:01:45.496750Z","iopub.status.idle":"2022-09-03T22:01:45.509023Z","shell.execute_reply.started":"2022-09-03T22:01:45.496696Z","shell.execute_reply":"2022-09-03T22:01:45.507692Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"\nCan we do better with another model?","metadata":{"_cell_guid":"6572777a-8c9c-1678-3651-65bd79577580"}},{"cell_type":"markdown","source":"### Decision Tree Model","metadata":{"_cell_guid":"2a7ce79c-2557-d306-977c-c44adde27c6b"}},{"cell_type":"code","source":"predictor_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']\nmodel = DecisionTreeClassifier()\nclassification_model(model,traindf,predictor_var,outcome_var)","metadata":{"_cell_guid":"36acf35e-296e-68fa-ab18-49967554af07","execution":{"iopub.status.busy":"2022-09-03T22:01:45.510867Z","iopub.execute_input":"2022-09-03T22:01:45.511733Z","iopub.status.idle":"2022-09-03T22:01:45.568328Z","shell.execute_reply.started":"2022-09-03T22:01:45.511684Z","shell.execute_reply":"2022-09-03T22:01:45.567024Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"Accuracy is improved but the model seems to suffer with validation scores. We are probably over-fitting.","metadata":{}},{"cell_type":"code","source":"# Make predictions on test set:\npredictions = model.predict(testdf[predictor_var])  \n# Print accuracy\naccuracy = metrics.accuracy_score(predictions,testdf[outcome_var])\nprint(\"Decision Tree Test Accuracy : %s\" % \"{0:.3%}\".format(accuracy))  ","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:01:45.571858Z","iopub.execute_input":"2022-09-03T22:01:45.572252Z","iopub.status.idle":"2022-09-03T22:01:45.584144Z","shell.execute_reply.started":"2022-09-03T22:01:45.572217Z","shell.execute_reply":"2022-09-03T22:01:45.583066Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{"_cell_guid":"40ca5482-3e1c-b8c1-2ac2-041897c9d160"}},{"cell_type":"code","source":"predictor_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\nclassification_model(model,traindf,predictor_var,outcome_var)","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:01:45.585553Z","iopub.execute_input":"2022-09-03T22:01:45.585931Z","iopub.status.idle":"2022-09-03T22:01:46.644602Z","shell.execute_reply.started":"2022-09-03T22:01:45.585894Z","shell.execute_reply":"2022-09-03T22:01:46.643326Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# Use all the features of the nucleus\npredictor_var = features_mean\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\nclassification_model(model, traindf,predictor_var,outcome_var)","metadata":{"_cell_guid":"e6edf958-508b-f46c-74a5-f1d8f421900f","execution":{"iopub.status.busy":"2022-09-03T22:01:46.645899Z","iopub.execute_input":"2022-09-03T22:01:46.646266Z","iopub.status.idle":"2022-09-03T22:01:47.730874Z","shell.execute_reply.started":"2022-09-03T22:01:46.646231Z","shell.execute_reply":"2022-09-03T22:01:47.729407Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"Using all nucleus features improve results. Training set predictions have an accuracy of 95.165%\nand cross-validation score of 93.626%. Now let's evaluate the model accuracy on the test set.","metadata":{}},{"cell_type":"code","source":"# Make predictions on test set:\npredictions = model.predict(testdf[predictor_var])  \n# Print accuracy\naccuracy = metrics.accuracy_score(predictions,testdf[outcome_var])\nprint(\"Random Forest Test Accuracy : %s\" % \"{0:.3%}\".format(accuracy))  ","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:01:47.733193Z","iopub.execute_input":"2022-09-03T22:01:47.733717Z","iopub.status.idle":"2022-09-03T22:01:47.763352Z","shell.execute_reply.started":"2022-09-03T22:01:47.733668Z","shell.execute_reply":"2022-09-03T22:01:47.762478Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"Random forest test accuracy is 92.982%.  An advantage with Random Forest is that it returns a feature importance matrix which can be used to select features. So lets select the top 5 features and use them as predictors for exploration.","metadata":{"_cell_guid":"6e6fb3f9-6fea-171f-fd18-b29d475c140c"}},{"cell_type":"code","source":"# Create a series with feature importances:\nfeatimp = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\nprint(featimp)","metadata":{"_cell_guid":"cc9f7243-0ad7-5d47-9791-a0be8757ff1c","execution":{"iopub.status.busy":"2022-09-03T22:01:47.764666Z","iopub.execute_input":"2022-09-03T22:01:47.765279Z","iopub.status.idle":"2022-09-03T22:01:47.783925Z","shell.execute_reply.started":"2022-09-03T22:01:47.765241Z","shell.execute_reply":"2022-09-03T22:01:47.782408Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# Using top 5 features\npredictor_var = ['concave points_mean','area_mean','radius_mean','perimeter_mean','concavity_mean',]\nmodel = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)\nclassification_model(model,traindf,predictor_var,outcome_var)","metadata":{"_cell_guid":"cb067a66-eea5-e2e3-ebc3-b18e832680e1","execution":{"iopub.status.busy":"2022-09-03T22:01:47.785817Z","iopub.execute_input":"2022-09-03T22:01:47.787290Z","iopub.status.idle":"2022-09-03T22:01:48.859723Z","shell.execute_reply.started":"2022-09-03T22:01:47.787216Z","shell.execute_reply":"2022-09-03T22:01:48.858314Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"Using the top 5 features returns worse accuracy and validation scores (94.286% and 92.088%) than when we used all nucleaus feauters above (95.165% and 93.626%).","metadata":{"_cell_guid":"cf8789ef-3898-7c7d-7e46-7526880d71c6"}},{"cell_type":"markdown","source":"## ANN","metadata":{}},{"cell_type":"code","source":"# Set the variables for training\nmax_epochs = 200\nbatch_size= 8\npatience = 10\n\n# Early stopping callback:\n# This callback will stop the training when there is no improvement in\n# the loss for given (patience) consecutive epochs.\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience)\n\ndef train_and_cross_validate_model(model, traindf, predictors, outcome):\n    #Perform k-fold cross-validation with 5 folds\n    kf = KFold(n_splits=5)\n    error = []\n    for train, val in kf.split(traindf):\n\n        # Training x\n        train_predictors = (traindf[predictors].iloc[train,:])    \n        # Training y\n        train_target = traindf[outcome].iloc[train]\n        # Val x\n        val_predictors = (traindf[predictors].iloc[val,:])  \n        # Val y\n        val_target = traindf[outcome].iloc[val]\n\n        # Training the algorithm using the predictors and target.\n        model.fit(\n            train_predictors,\n            train_target,\n            epochs=max_epochs,\n            batch_size=batch_size,\n            validation_data=(val_predictors, val_target),\n            callbacks=[callback]\n        )\n\n        # Print error from each cross-validation run\n        scores = model.evaluate(val_predictors, val_target, verbose=0)\n        # scores[0] is loss whereas scores[1] is accuracy\n        print(\"Cross-Validation Score from current fold: %s\" % \"{0:.3%}\".format(scores[1]))\n\n        # Record error from each cross-validation run\n        error.append(scores[1])\n\n    # Make predictions on training set and calculate accuracy\n    loss, accuracy = model.evaluate(traindf[predictors], traindf[outcome]) \n    \n    print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))  \n    print(\"Score : %s\" % \"{0:.3%}\".format(np.mean(error)))","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:01:49.786843Z","iopub.execute_input":"2022-09-03T22:01:49.787435Z","iopub.status.idle":"2022-09-03T22:01:49.801438Z","shell.execute_reply.started":"2022-09-03T22:01:49.787378Z","shell.execute_reply":"2022-09-03T22:01:49.799364Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"\nWe will create a Neural Network that has one hidden layer:\n* Input layer has 10 nodes for all nucleus features. The activation function for this layer is Rectified Linear Units (ReLU). ReLU is a half rectified function; that is, for all the inputs less than 0 the value is 0 while for anything positive the value is retained.\n* Hidden layer consists of 8 nodes. The activation function for this layer is ReLU.\n* Output layer has 1 node. It uses the sigmoid activation function that will squeeze all the values between 0 and 1 into the form of a sigmoid curve.One output unit is used since for each record values in X, a probability will be predicted. If it is high ( >0.9) than the tumor is definitely malignant. If it is less ( <0.2) then it is definitely benign.\n\n","metadata":{}},{"cell_type":"code","source":"# use all features\npredictor_var = features_mean\nfeature_size = len(predictor_var)\n\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(feature_size,)),\n    keras.layers.Dense(5, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:01:49.803373Z","iopub.execute_input":"2022-09-03T22:01:49.804262Z","iopub.status.idle":"2022-09-03T22:01:49.858538Z","shell.execute_reply.started":"2022-09-03T22:01:49.804199Z","shell.execute_reply":"2022-09-03T22:01:49.857192Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"Model has been trained with 5-fold cross validation. By using corss valdiation, whole training set has been used on top of early stopping based on model prediction accuracy on the validation set. Training set prediction accuracy is 89.231% and mean validation set prediction accuracy is 89.011%. Now, let's evaluate the NN's performance on our test set.","metadata":{}},{"cell_type":"code","source":"# test\ntest_loss, test_acc = model.evaluate(testdf[predictor_var], testdf[outcome_var])\n    \nprint(\"NN Test Accuracy : %s\" % \"{0:.3%}\".format(test_acc))  ","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:02:28.096166Z","iopub.execute_input":"2022-09-03T22:02:28.096541Z","iopub.status.idle":"2022-09-03T22:02:28.187938Z","shell.execute_reply.started":"2022-09-03T22:02:28.096506Z","shell.execute_reply":"2022-09-03T22:02:28.186687Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"model.save(\"breast_cancer_nn.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-09-03T22:41:41.853588Z","iopub.execute_input":"2022-09-03T22:41:41.854071Z","iopub.status.idle":"2022-09-03T22:41:41.874161Z","shell.execute_reply.started":"2022-09-03T22:41:41.854025Z","shell.execute_reply":"2022-09-03T22:41:41.872922Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nThe best model to be used for diagnosing breast cancer as found in this analysis is the NN based on holdout test results. Using all nucleaus features for training yields better results than as with less features.","metadata":{"_cell_guid":"0310c568-10fc-b19d-b2ee-c87a8f3f3280"}},{"cell_type":"markdown","source":"The prediction accuracy for the test data set using the:\n1. logistic regression model is 90.351%.\n2. random forest model is 92.982%.\n3. decision tree model is 92.982%.\n4. NN is 93.860%\n","metadata":{}}]}